## 卷积优化   
## 为什么可以用矩阵乘法优化卷积操作？

矩阵乘法优化卷积操作的核心思想是将卷积操作转换为矩阵乘法，从而利用现代硬件（如 CPU/GPU）对矩阵乘法的高效支持。下面我们通过一个具体的例子来说明为什么可以用矩阵乘法优化卷积操作。

---

### 1. 卷积操作的朴素实现

假设我们有一个输入特征图 $Z$ 和一个卷积核 $W$：
- 输入 $Z$ 的形状为 $(H, W, C_{in})$，其中 $H$ 是高度，$W$ 是宽度，$C_{in}$ 是输入通道数。
- 卷积核 $W$ 的形状为 $(K, K, C_{in}, C_{out})$，其中 $K$ 是卷积核大小，$C_{out}$ 是输出通道数。

朴素卷积的实现是通过滑动窗口逐元素计算：
- 对于输出特征图的每个位置 $(y, x)$，从输入 $Z$ 中提取一个大小为 $(K, K, C_{in})$ 的局部区域（感受野）。
- 将这个局部区域与卷积核 $W$ 逐元素相乘并求和，得到输出的一个值。

这种实现需要多层嵌套循环，计算效率较低。

---

### 2. 矩阵乘法的优化思路

矩阵乘法的优化思路是将卷积操作转换为矩阵乘法。具体步骤如下：

#### 2.1 将输入展开为矩阵
- 将输入 $Z$ 的每个局部区域（感受野）展开为矩阵的一行。
- 假设输出特征图的大小为 $(H_{out}, W_{out})$，则展开后的矩阵形状为 $(H_{out} \cdot W_{out}, K \cdot K \cdot C_{in})$。

#### 2.2 将卷积核展开为矩阵
- 将卷积核 $W$ 展开为矩阵的列。
- 展开后的矩阵形状为 $(K \cdot K \cdot C_{in}, C_{out})$。

#### 2.3 矩阵乘法
- 将展开后的输入矩阵与卷积核矩阵相乘，得到输出矩阵。
- 输出矩阵的形状为 $(H_{out} \cdot W_{out}, C_{out})$，可以重新调整为 $(H_{out}, W_{out}, C_{out})$。

---

### 3. 具体例子

假设：
- 输入 $Z$ 的形状为 $(4, 4, 1)$（即 $H=4, W=4, C_{in}=1$）。
- 卷积核 $W$ 的形状为 $(3, 3, 1, 1)$（即 $K=3, C_{out}=1$）。
- 输出特征图的形状为 $(2, 2, 1)$（即 $H_{out}=2, W_{out}=2, C_{out}=1$）。

#### 3.1 输入展开为矩阵
- 输入 $Z$ 的局部区域（感受野）为 $3 \times 3$，共有 $2 \times 2 = 4$ 个局部区域。
- 将每个局部区域展开为一行，得到一个形状为 $(4, 9)$ 的矩阵。

例如：  
Z = [[1, 2, 3, 4],  
[5, 6, 7, 8],  
[9, 10, 11, 12],  
[13, 14, 15, 16]]  

展开后的矩阵：  
[[1, 2, 3, 5, 6, 7, 9, 10, 11], # 第一个局部区域  
[2, 3, 4, 6, 7, 8, 10, 11, 12], # 第二个局部区域  
[5, 6, 7, 9, 10, 11, 13, 14, 15], # 第三个局部区域  
[6, 7, 8, 10, 11, 12, 14, 15, 16]] # 第四个局部区域  

#### 3.2 卷积核展开为矩阵
- 卷积核 \(W\) 的形状为 \((3, 3, 1, 1)\)，展开为形状为 \((9, 1)\) 的矩阵。

例如：  
W = [[1, 0, -1],  
[1, 0, -1],  
[1, 0, -1]]  

展开后的矩阵：  
[[1],  
[1],  
[1],  
[0],  
[0],  
[0],  
[-1],  
[-1],  
[-1]]    
#### 3.3 矩阵乘法
- 将展开后的输入矩阵与卷积核矩阵相乘： 
输出矩阵 = 输入矩阵 @ 卷积核矩阵  

- 结果是一个形状为 \((4, 1)\) 的矩阵，可以重新调整为 \((2, 2, 1)\) 的输出特征图。

---

### 4. 代码实现

以下是使用矩阵乘法优化卷积操作的代码示例：

```python
import numpy as np

def conv_matrix_mult(Z, weight):
    H, W, C_in = Z.shape
    K, _, _, C_out = weight.shape
    H_out = H - K + 1
    W_out = W - K + 1
    
    # 将输入展开为矩阵
    Z_matrix = np.zeros((H_out * W_out, K * K * C_in))
    idx = 0
    for y in range(H_out):
        for x in range(W_out):
            Z_slice = Z[y:y+K, x:x+K, :].flatten()
            Z_matrix[idx, :] = Z_slice
            idx += 1
    
    # 将卷积核展开为矩阵
    W_matrix = weight.reshape(K * K * C_in, C_out)
    
    # 矩阵乘法
    out_matrix = Z_matrix @ W_matrix
    
    # 调整输出形状
    out = out_matrix.reshape(H_out, W_out, C_out)
    return out

# 示例输入
Z = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12],
              [13, 14, 15, 16]], dtype=np.float32).reshape(4, 4, 1)
weight = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]], dtype=np.float32).reshape(3, 3, 1, 1)

# 卷积操作
out = conv_matrix_mult(Z, weight)
print(out)   
```   
#### 5.总结   
关键在于把输入展开成:  
行:单个输入矩阵,感受野的数量   
行向量:感受野矩阵的一维展开 x 输入通道数  
  
卷积核展开成:(存在转置)  
列:卷积核输出通道数  
列向量:卷积核的一维矩阵展开 x 输入通道数



